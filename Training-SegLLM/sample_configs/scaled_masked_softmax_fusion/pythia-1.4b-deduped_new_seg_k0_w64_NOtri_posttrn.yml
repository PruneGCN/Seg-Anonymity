{
  # parallelism
  "pipe-parallel-size": 1,
  "model-parallel-size": 1,

  # model settings
  "num-layers": 24,
  "hidden-size": 2048,
  "num-attention-heads": 16,
  "seq-length": 2048,
  "max-position-embeddings": 2048,
  "pos-emb": "rotary",
  "rotary-pct": 0.25,
  "no-weight-tying": true,
  "gpt-j-residual": true,
  "output-layer-parallelism": "column",
  
  # "attention-config": [[["flash"], 24]],
  "attention-config": [[["global"], 24]],  #my

  # "scaled-upper-triang-masked-softmax-fusion": true,
  "scaled_masked_softmax_fusion": true, ####my !!!!
  "bias-gelu-fusion": true,

  # init methods
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",

  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.0002,
      "betas": [0.9, 0.95],
      "eps": 1.0e-8
    }
  },
  "min_lr": 0.00002,

  "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 500000000,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 500000000,
    "contiguous_gradients": true,
    "cpu_offload": false
  },

  # batch size (trained on 64 gpus)
  "train_micro_batch_size_per_gpu": 32,  # default 16
  "gradient_accumulation_steps": 8,  # default 1
  # "gas": 1,
  "data-impl": "mmap",
  "num_workers": 1,

  # activation checkpointing
  "checkpoint-activations": true,
  "checkpoint-num-layers": 1,
  "partition-activations": true,
  "synchronize-each-layer": true,

  # regularization
  "gradient_clipping": 1.0,
  "weight-decay": 0.1,
  "hidden-dropout": 0,
  "attention-dropout": 0,

  # precision settings
  "fp16": {
    "fp16": true,
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 12,
    "hysteresis": 2,
    "min_loss_scale": 1,
  },

  "train-iters": 143000,
  "lr-decay-iters": 143000,
  "distributed-backend": "nccl",
  "lr-decay-style": "cosine",
  "warmup": 0.01,
  "checkpoint-factor": 1000,
  "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],
  "eval-interval": 4000, # default 40000
  "eval-iters": 10,

  "log-interval": 10,
  "steps_per_print": 10,
  "wall_clock_breakdown": true,

  "train-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],
  "valid-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],
  "test-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],



  "tokenizer-type": "HFTokenizer",
  "vocab-file": "/home/txiao/shihan/workspace/gpt-neox-new_seg/my_pythia_configs/20B_tokenizer.json",

  "launcher": "pdsh",
  # "deepspeed_slurm": true,

  "save": "/lustre/fast/fast/txiao/shihan/saves/gpt-neox-new_seg_posttrn/pythia-1.4b-deduped/converted_seq_checkpoints/posttraining/four_cards/seg_k0_w64",
  "load": "/lustre/fast/fast/txiao/shihan/saves/gpt-neox-new_seg_posttrn/pythia-1.4b-deduped/converted_seq_checkpoints/posttraining/four_cards/seg_k0_w64",
  # "load": "/lustre/fast/fast/txiao/shihan/saves/gpt-neox-new_seg_posttrn/pythia-1.4b-deduped/converted_seq_checkpoints/four_cards/",

  ## my:
  ####################################################################### my #######################################################################:
  # "hostfile": "/home/txiao/shihan/workspace/gpt-neox-new_seg/my_pythia_configs/hostfile",

  'special_tokens_id': [15, 13, 32, 2, 28, 27, 209, 186, 187],  ## Fixed
  'PADDING_ID': 0 , # pythia  ## Fixed
  
  'prefill_window_size' : 64,  # The local window size when prefilling (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_PREFILL_WINDOW_SIZE=False
  'decode_window_size' : 64,   # The local window size when generating (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_DECODE_WINDOW_SIZE=False        

  'USE_DYNAMIC_PREFILL_WINDOW_SIZE':   False,  # If True: the prefilling window sizes for different decoder layers are different; If True: should set 'prefill_win_size_list', else: should set 'prefill_window_size'
  'USE_DYNAMIC_DECODE_WINDOW_SIZE' :   False,  # If True: the decoding window sizes for different decoder layers are different;   If True: should set 'decode_win_size_list', else: should set 'decode_window_size' 
  

  'prefill_win_size_list' : [64,   64,   64,   64,   64,   64,  64,   64,  64, 64,
                             64,     64,   64,   64,   64,   64,  64,   64,  64, 64,
                             64,     64,   64,   64],  

  # 'prefill_win_size_list' :[20,   19,   18,   17,   16,   15,  14,   13,  12, 11, 10,  9 ], 
                                                                

  'decode_win_size_list': [64,   64,   64,   64,   64,   64,  64,   64,  64, 64,
                           64,     64,   64,   64,   64,   64,  64,   64,  64, 64,
                           64,     64,   64,   64],  

                            
  # 'decode_win_size_list': [20,   19,   18,   17,   16,   15,  14,   13,  12, 11, 10,  9 ],                             

  'att_sink_max_idx' :  2, # The largest index for attention sink tokens

  ######################################There should be at most 1 True for the following 4 args ##############################################
  'original_flag' : False,   # Run the pretrained model without any modification if True
  'streamingLLM' :  False,  ## NOT implemented yet  # Run streamingLLM. Only takes effect when self.original_flag=False
  'random_special_tokens_uniform': False, ## NOT implemented yet # Keep random selected tokens to replace seps. Select one other token between two seps
  'random_special_tokens_general': False, ## NOT implemented yet # Keep random selected tokens to replace seps. Randomly choose tokens generally
  ######################################There should be at most 1 True for the above 4 args ##############################################

  'BATCH_DYNAMIC_ATT_SINK' : False,  # If True: use the floating attension sink positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding).  Can be False when pretraining since the attention sinks are at the beginning of each seq in batch for pretraining (i.e., right padding)
  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for generating). When pretraining, it will print the retention ratio for the computational complexity of calculating the attention map if it is set True
  'print_KV_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_KV_intervals' forward passes (or print_KV_intervals/gas  iterations). It only takes effect when PRINT_KV_RATIO=True.    
  ####################################################################### ### #######################################################################

}
