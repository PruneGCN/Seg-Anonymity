# GPT-2 pretraining setup
{
   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages
   # across the node boundaries )
   "pipe_parallel_size": 1,
   "model_parallel_size": 1,

   # model settings
   "num_layers": 12,
   "hidden_size": 768,
   "num_attention_heads": 12,
   "seq_length": 2048,
   "max_position_embeddings": 2048,
   "norm": "layernorm",
   "pos_emb": "rotary",
   "no_weight_tying": true,  ## True for pythia
   "gpt_j_residual": false,  ## True for pythia
   "output_layer_parallelism": "column",

   # these should provide some speedup but takes a while to build, set to true if desired
  #  "scaled_upper_triang_masked_softmax_fusion": false,
   "scaled_upper_triang_masked_softmax_fusion": true, ##my
   "bias_gelu_fusion": false,  ## True for pythia
   "rope_fusion": false,   ## False for pythia
   "layernorm_fusion": false,  # False for pythia

   # init methods
   "init_method": "small_init",
   "output_layer_init_method": "wang_init",


   # optimizer settings
   "optimizer": {
     "type": "Adam",
     "params": {
       "lr": 0.0006,
       "betas": [0.9, 0.95],
       "eps": 1.0e-8,
     }
   },
   "min_lr": 0.00006,

   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training
   "zero_optimization": {
    "stage": 1,
    "allgather_partitions": True,
    "allgather_bucket_size": 500000000,
    "overlap_comm": True,
    "reduce_scatter": True,
    "reduce_bucket_size": 500000000,
    "contiguous_gradients": True,
  },

   # batch / data settings
  #  "train_micro_batch_size_per_gpu": 4, ##default
  "train_micro_batch_size_per_gpu": 32, #32 
  "gradient_accumulation_steps": 4,  #4
   "data_impl": "mmap",

   # activation checkpointing
   "checkpoint_activations": true,
   "checkpoint_num_layers": 1,
   "partition_activations": true,
   "synchronize_each_layer": true,

   # regularization
   "gradient_clipping": 1.0,
   "weight_decay": 0.1,
   "hidden_dropout": 0.0,
   "attention_dropout": 0.0,

   # precision settings
   "fp16": {
     "enabled": true,
     "loss_scale": 0,
     "loss_scale_window": 1000,
     "hysteresis": 2,
     "min_loss_scale": 1
   },

   # misc. training settings
  #  "train_iters": 320000,  ##default
  #  "lr_decay_iters": 320000,  ##default
   "train_iters": 143000,  ##my
   "lr_decay_iters": 143000,  ##my


   "distributed_backend": "nccl",
   "lr_decay_style": "cosine",
   "warmup": 0.01,
   "checkpoint_factor": 1000, ##my. default 10000
   "eval_interval": 4000,  ##my. default 1000
   "eval_iters": 10,

   # logging
   "log_interval": 10, ##my. default 100
   "steps_per_print": 10,
   "keep_last_n_checkpoints": 4,
   "wall_clock_breakdown": true,

  #  networking
  # "hostfile": "/mock_path" ##my comment
  #######################################################################################################################
  "train-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],
  "valid-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],
  "test-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],

  "tokenizer-type": "HFTokenizer",
  "vocab-file": "/home/txiao/shihan/workspace/gpt-neox-new_seg_bipe_SA/my_pythia_configs/20B_tokenizer.json",

  "launcher": "pdsh",
  #"deepspeed_slurm": true,
  #"deepspeed": false

  "save": "/lustre/fast/fast/txiao/shihan/saves/gpt-neox-new_seg_bipe_SA/pythia125m-deduped/scaled-upper-triang-masked-softmax-fusion/checkpoints125m_w2048_8cards_SA",
  "load": "/lustre/fast/fast/txiao/shihan/saves/gpt-neox-new_seg_bipe_SA/pythia125m-deduped/scaled-upper-triang-masked-softmax-fusion/checkpoints125m_w2048_8cards_SA",

  ## my:
  ####################################################################### my #######################################################################:
  # "hostfile": "/home/txiao/shihan/workspace/gpt-neox-new_seg/my_pythia_configs/hostfile",

  'special_tokens_id': [15, 13, 32, 2, 28, 27, 209, 186, 187],  ## Fixed
  'PADDING_ID': 0 , # For pythia  ## Fixed
  
  'prefill_window_size' : 2048,  # The local window size when prefilling (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_PREFILL_WINDOW_SIZE=False
  'decode_window_size' : 2048,   # The local window size when generating (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_DECODE_WINDOW_SIZE=False        

  'USE_DYNAMIC_PREFILL_WINDOW_SIZE':   False,  # If True: the prefilling window sizes for different decoder layers are different; If True: should set 'prefill_win_size_list', else: should set 'prefill_window_size'
  'USE_DYNAMIC_DECODE_WINDOW_SIZE' :   False,  # If True: the decoding window sizes for different decoder layers are different;   If True: should set 'decode_win_size_list', else: should set 'decode_window_size' 
  

  'prefill_win_size_list' : [2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,
                             2048,   2048],  

  # 'prefill_win_size_list' :[20,   19,   18,   17,   16,   15,  14,   13,  12, 11, 10,  9 ], 
                                                                

  'decode_win_size_list': [2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,   2048,
                           2048,   2048],   
                            
  # 'decode_win_size_list': [20,   19,   18,   17,   16,   15,  14,   13,  12, 11, 10,  9 ],                             

  'att_sink_max_idx' :  2, # The largest index for attention sink tokens

  'USE_SA_SOFTMAX': True, # If True, use Self-Adjusting Softmax Attention.
  'USE_BiPE':  False,  # If True (must also set pos_emb='rotary' or 'alibi'), use Bilevel Positional Encoding.
  'BiPE_seps': [15, 13, 32, 2, 28, 27, 209, 186, 187],  # The token ids of the seperator tokens for BiPE:  ['.', ',', '?', '!', ';', ":", ' ', '\t','\n'] 
  ######################################There should be at most 1 True for the following 4 args ##############################################
  'original_flag' : True,  # Train the model without SegLLM's modification if True. Other modifications depend on other parameters' setting such as USE_SA_SOFTMAX.
  'streamingLLM' :  False,  ## NOT implemented yet  # Run streamingLLM. Only takes effect when self.original_flag=False
  'random_special_tokens_uniform': False, ## NOT implemented yet # Keep random selected tokens to replace seps. Select one other token between two seps
  'random_special_tokens_general': False, ## NOT implemented yet # Keep random selected tokens to replace seps. Randomly choose tokens generally
  ######################################There should be at most 1 True for the above 4 args ##############################################

  'BATCH_DYNAMIC_ATT_SINK' : False,  # If True: use the floating attension sink positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding).  Can be False when pretraining since the attention sinks are at the beginning of each seq in batch for pretraining (i.e., right padding)
  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for generating). When pretraining, it will print the retention ratio for the computational complexity of calculating the attention map if it is set True
  'print_KV_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_KV_intervals' forward passes (or print_KV_intervals/gas  iterations). It only takes effect when PRINT_KV_RATIO=True.    
  ####################################################################### ### #######################################################################



}
